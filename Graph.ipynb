{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list for the graph\n",
    "edge_list = [['Mannheim', 'Frankfurt', 85], ['Mannheim', 'Karlsruhe', 80], ['Erfurt', 'Wurzburg', 186], ['Munchen', 'Numberg', 167], ['Munchen', 'Augsburg', 84], ['Munchen', 'Kassel', 502], ['Numberg', 'Stuttgart', 183], ['Numberg', 'Wurzburg', 103], ['Numberg', 'Munchen', 167], ['Stuttgart', 'Numberg', 183], ['Augsburg', 'Munchen', 84], ['Augsburg', 'Karlsruhe', 250], ['Kassel', 'Munchen', 502], ['Kassel', 'Frankfurt', 173], ['Frankfurt', 'Mannheim', 85], ['Frankfurt', 'Wurzburg', 217], ['Frankfurt', 'Kassel', 173], ['Wurzburg', 'Numberg', 103], ['Wurzburg', 'Erfurt', 186], ['Wurzburg', 'Frankfurt', 217], ['Karlsruhe', 'Mannheim', 80], ['Karlsruhe', 'Augsburg', 250],[\"Mumbai\", \"Delhi\",400],[\"Delhi\", \"Kolkata\",500],[\"Kolkata\", \"Bangalore\",600],[\"TX\", \"NY\",1200],[\"ALB\", \"NY\",800]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a graph\n",
    "graph = nx.Graph()\n",
    "\n",
    "#Fill graph using edgelist\n",
    "for edge in edge_list:\n",
    "    graph.add_edge(edge[0], edge[1], weight = edge[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find different sections of the graph that are connected\n",
    "for group, node in enumerate(nx.connected_components(graph)):\n",
    "    print(\"cc\" + str(group) + \":\", node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find shortest path and shortest path length\n",
    "shortest_path = nx.shortest_path(graph, 'Stuttgart', 'Frankfurt', weight='weight')\n",
    "shortest_path_length = nx.shortest_path_length(graph, 'Stuttgart', 'Frankfurt', weight='weight')\n",
    "\n",
    "print(shortest_path)\n",
    "print(shortest_path_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shortest path between all pairs\n",
    "for shortest_path_i in nx.all_pairs_dijkstra_path(graph, weight='weight'):\n",
    "    print(shortest_path_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a smaller graph\n",
    "edge_list = [['Mannheim', 'Frankfurt', 85], ['Mannheim', 'Karlsruhe', 80], ['Erfurt', 'Wurzburg', 186], ['Munchen', 'Numberg', 167], ['Munchen', 'Augsburg', 84], ['Munchen', 'Kassel', 502], ['Numberg', 'Stuttgart', 183], ['Numberg', 'Wurzburg', 103], ['Numberg', 'Munchen', 167], ['Stuttgart', 'Numberg', 183], ['Augsburg', 'Munchen', 84], ['Augsburg', 'Karlsruhe', 250], ['Kassel', 'Munchen', 502], ['Kassel', 'Frankfurt', 173], ['Frankfurt', 'Mannheim', 85], ['Frankfurt', 'Wurzburg', 217], ['Frankfurt', 'Kassel', 173], ['Wurzburg', 'Numberg', 103], ['Wurzburg', 'Erfurt', 186], ['Wurzburg', 'Frankfurt', 217], ['Karlsruhe', 'Mannheim', 80]]\n",
    "graph = nx.Graph()\n",
    "\n",
    "#Fill graph using edgelist\n",
    "for edge in edge_list:\n",
    "    graph.add_edge(edge[0], edge[1], weight = edge[2])\n",
    "\n",
    "nx.draw_networkx(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the minimum spanning tree (MST)\n",
    "mst_graph = nx.minimum_spanning_tree(graph)\n",
    "\n",
    "nx.draw_networkx(mst_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possibili Analisi\n",
    "# - Similarità tra post twitter (identificazione post simili, stesso topic, etc.)\n",
    "\n",
    "# - Grafo relazioni utenti (come sono collegati gli utenti di cui raccogliamo i dati? casuali e non si conoscono o ci sono \"gruppi di conoscenze?\")\n",
    "\n",
    "# - Partendo dai post pubblicati in una determinata data/settimana/periodo\n",
    "#   - Calcolo score associato a sentiment analysis (average?)\n",
    "#   - Distanza tra due articoli = Delta tra i valori di sentiment analysis\n",
    "\n",
    "# - Comparazione grafo relazioni e grafo sentiment (i gruppi di persone in contatto tra loro condividono sentimenti comuni?)\n",
    "\n",
    "# - Analisi link in pagine web (per vedere quale fonte viene citata di più) (Pagerank)\n",
    "#   - più collegamenti = più affidabili? (Dipende)\n",
    "\n",
    "# - Grafo analisi triple (soggetto, predicato, complemento)\n",
    "#   - Necessia pre-processing per standardizzare le parti (a quanto ho potuto vedere non sempre sono simili)\n",
    "#     - e.g. ('man', 'gather', 'rock') e ('man', 'gather', 'big rock') possono essere estratti entrambi (meglio tenere il + semplice?)\n",
    "#   - Per ogni soggetto che si vuole analizzare (e.g. Unione Europea, etc.)\n",
    "#     - Grafo dove è SUBJECT con link a verbi e complemento oggetto\n",
    "#     - Grafo dove è COBJECT con link a verbi e soggetto\n",
    "\n",
    "# - Grafo analisi nomi (Dato un nome, osserviamo con quali altri nomi è collegato e come quest'ultimi sono collegati tra loro, e.g. collegamenti con verbi o\n",
    "#                       tipologie specifiche di nomi)\n",
    "#   - Quali sono gli altri nomi più rilevanti (rispetto a quello scelto)? (centralità)\n",
    "\n",
    "# - Collegamento tra topic in base ai link degli articoli\n",
    "# - \n",
    "# - \n",
    "# - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "node_list = []\n",
    "numero_articolo = 0\n",
    "\n",
    "for i in range (0, 100):\n",
    "    numero_articolo = numero_articolo + 1\n",
    "    score = random.randint(-100, 100)\n",
    "    node = [str(numero_articolo), score]\n",
    "    node_list.append(node)\n",
    "\n",
    "edge_list = []\n",
    "for node_start in node_list:\n",
    "    for node_end in node_list:\n",
    "        if(node_start != node_end and abs(node_start[1] - node_end[1]) < 5):\n",
    "            edge = [node_start[0], node_end[0], abs(node_start[1] - node_end[1])]\n",
    "            edge_list.append(edge)\n",
    "            \n",
    "sentiment_distance_graph = nx.Graph()\n",
    "\n",
    "for edge in edge_list:\n",
    "    sentiment_distance_graph.add_edge(edge[0], edge[1], weight = edge[2])\n",
    "    \n",
    "#nx.draw_networkx(sentiment_distance_graph)\n",
    "\n",
    "pos = nx.spring_layout(sentiment_distance_graph)\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (20, 15)\n",
    "plt.axis('off')\n",
    "\n",
    "nx.draw_networkx(sentiment_distance_graph, pos, with_labels = False, node_size = 35, edge_color = '#D3D3D3')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json(\"./doc_topic.json\")\n",
    "\n",
    "data_organized = []\n",
    "\n",
    "#Converts input from json to list\n",
    "for i in range(0, data.shape[1]):\n",
    "    topic = data[i].topic[0][0]\n",
    "    words = []\n",
    "    \n",
    "    number_tmp = \"\"\n",
    "    word_tmp = \"\"\n",
    "    word_element_tmp = []\n",
    "    \n",
    "    word_string = data[i].words[1]\n",
    "    for char in word_string:\n",
    "        if(char.isdigit() or char == '.'):\n",
    "            number_tmp = number_tmp + char\n",
    "        elif char == '*':\n",
    "            word_element_tmp.append(float(number_tmp))\n",
    "            number_tmp = \"\"\n",
    "        elif char == '\"' and word_tmp == \"\":\n",
    "            continue\n",
    "        elif char == '\"' and word_tmp != \"\":\n",
    "            word_element_tmp.append(word_tmp)\n",
    "            word_tmp = \"\"\n",
    "            words.append(word_element_tmp)\n",
    "            word_element_tmp = []\n",
    "        elif char != '+' and char != ' ':\n",
    "            word_tmp = word_tmp + char\n",
    "    \n",
    "    data_organized.append([topic, words]) \n",
    "\n",
    "list_of_all_topics = []\n",
    "\n",
    "#Extract the name of all the topics found (in this case the number)\n",
    "for d in data_organized:\n",
    "    if(d[0] not in list_of_all_topics):\n",
    "        list_of_all_topics.append(d[0])\n",
    "        \n",
    "list_of_all_topics.sort()\n",
    "\n",
    "data_per_topic = []\n",
    "for topic in list_of_all_topics:\n",
    "    all_words_for_topic = []\n",
    "    document_count = 0\n",
    "#   Unisce in un unico array tutte le parole associate\n",
    "    for dat in (d for d in data_organized if d[0] == topic):\n",
    "        document_count = document_count + 1;\n",
    "        for word in dat[1]:\n",
    "            all_words_for_topic.append(word)\n",
    "    \n",
    "    print(all_words_for_topic[2])\n",
    "    \n",
    "    all_words_for_topic_once_not_averaged = []\n",
    "    all_words_for_topic_no_prob = []\n",
    "    \n",
    "    #Sum of all the probabilities for each word\n",
    "    for word in all_words_for_topic:\n",
    "        if(word[1] not in all_words_for_topic_no_prob):\n",
    "            all_words_for_topic_no_prob.append(word[1])\n",
    "            all_words_for_topic_once_not_averaged.append(word)\n",
    "        else:\n",
    "            for w in all_words_for_topic_once_not_averaged:\n",
    "                if(w[1] == word[1]):\n",
    "                    w[0] = w[0] + word[0]\n",
    "    \n",
    "    sum_val = 0;\n",
    "    #Average of how many times the word appears in the topic\n",
    "    for word in all_words_for_topic_once_not_averaged:\n",
    "        word[0] = round(word[0] / document_count, 4)\n",
    "        sum_val = sum_val + word[0]\n",
    "        \n",
    "    #print(all_words_for_topic_once_not_averaged)\n",
    "    data_per_topic.append(all_words_for_topic_once_not_averaged)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing word graphs\n",
    "import networkx as nx\n",
    "\n",
    "node_list = [('A'), ('B'), ('C')]\n",
    "edge_list = [('A', 'B', {'action': [('hit', 1), ('fall', 1)]}), ('A', 'C', {'action': 'hug', 'frequency': 1}), ('B', 'C', {'action': 'hit', 'frequency': 1})]\n",
    "\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "graph.add_nodes_from(node_list)\n",
    "graph.add_edges_from(edge_list)\n",
    "    \n",
    "nx.draw_networkx(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "#From Triples to a graph\n",
    "class TripleGraph:\n",
    "    node_list_set = []\n",
    "    edge_list = []\n",
    "    input_triples = []\n",
    "    weight_list = []\n",
    "    graph = nx.DiGraph()\n",
    "    \n",
    "    def __init__(self, input_triples):\n",
    "        self.input_triples = input_triples\n",
    "        \n",
    "    def resetGraph(self):\n",
    "        self.graph.clear()\n",
    "        self.node_list_set = []\n",
    "        self.edge_list = []\n",
    "        self.weight_list = []\n",
    "    \n",
    "    def createAttributeGraph(self):\n",
    "        #reset Graph\n",
    "        self.resetGraph()\n",
    "        #create nodes\n",
    "        node_list = []\n",
    "        for i_t in self.input_triples:\n",
    "            node_list.append(i_t[0]);\n",
    "            node_list.append(i_t[2]);\n",
    "\n",
    "        self.node_list_set = set(node_list)\n",
    "\n",
    "        self.graph.add_nodes_from(self.node_list_set)\n",
    "\n",
    "        for i_t in self.input_triples:\n",
    "\n",
    "            found = False\n",
    "            position = 0\n",
    "\n",
    "            #Look for existing relation\n",
    "            for edge in self.edge_list:\n",
    "                if i_t[0] == edge[0] and i_t[2] == edge[1]:\n",
    "                    found = True\n",
    "                    break\n",
    "                position = position + 1\n",
    "\n",
    "            #Update Element        \n",
    "            if found == True:\n",
    "\n",
    "                verb_found = False\n",
    "                verb_position = 0\n",
    "\n",
    "                #Looking for Position to Update\n",
    "                for edge_verb in self.edge_list[position][2]:\n",
    "                    if edge_verb[0] == i_t[1]:\n",
    "                        verb_found = True\n",
    "                        break\n",
    "                    verb_position = verb_position + 1\n",
    "\n",
    "                #Same verb\n",
    "                if verb_found == True:\n",
    "                    update_element = self.edge_list[position][2].pop(verb_position)\n",
    "                    self.edge_list[position][2].append((update_element[0], update_element[1] + 1))\n",
    "                #Different verb\n",
    "                else:\n",
    "                    self.edge_list[position][2].append((i_t[1], 1))\n",
    "            #New Element\n",
    "            else:\n",
    "                self.edge_list.append((i_t[0], i_t[2], [(i_t[1], 1)]))\n",
    "\n",
    "        self.graph.add_edges_from(self.edge_list)\n",
    "    \n",
    "    def getNodeOutgoingRelations(self, node_name):\n",
    "        \n",
    "        print('Outgoing Relations of ' + str(node_name))\n",
    "        for edge in self.edge_list:\n",
    "            if edge[0] == node_name:\n",
    "                for action in edge[2]:\n",
    "                    print(edge[0] + ' --> ' + action[0] + ' (Frequency = ' + str(action[1]) + ') --> ' + edge[1])\n",
    "                \n",
    "    def getNodeIngoingRelations(self, node_name):\n",
    "        print('Ingoing Relations of ' + str(node_name))\n",
    "        \n",
    "        for edge in self.edge_list:\n",
    "            if edge[1] == node_name:\n",
    "                for action in edge[2]:\n",
    "                    print(edge[0] + ' --> ' + action[0] + ' (Frequency = ' + str(action[1]) + ') --> ' + edge[1])\n",
    "    \n",
    "    def createWeightedGraph(self):        \n",
    "        #reset Graph\n",
    "        self.resetGraph()\n",
    "        \n",
    "        #create nodes\n",
    "        node_list = []\n",
    "        for i_t in self.input_triples:\n",
    "            node_list.append(i_t[0]);\n",
    "            node_list.append(i_t[2]);\n",
    "\n",
    "        self.node_list_set = set(node_list)\n",
    "\n",
    "        self.graph.add_nodes_from(self.node_list_set)\n",
    "        \n",
    "        for i_t in self.input_triples:\n",
    "            \n",
    "            found = False\n",
    "            position = 0\n",
    "\n",
    "            #Look for existing relation\n",
    "            for edge in self.edge_list:\n",
    "                if i_t[0] == edge[0] and i_t[2] == edge[1]:\n",
    "                    found = True\n",
    "                    break\n",
    "                position = position + 1\n",
    "\n",
    "            #Update Element        \n",
    "            if found == True:\n",
    "                self.weight_list[position] = self.weight_list[position] + 1\n",
    "            #New Element\n",
    "            else:\n",
    "                self.edge_list.append((i_t[0], i_t[2]))\n",
    "                self.weight_list.append(1)\n",
    "\n",
    "        i = 0\n",
    "        tmp_edge_list = []\n",
    "        for edge in self.edge_list:\n",
    "            tmp_edge_list.append(edge + (self.weight_list[0],))\n",
    "\n",
    "        self.edge_list = tmp_edge_list\n",
    "        \n",
    "        self.graph.add_weighted_edges_from(self.edge_list)\n",
    "\n",
    "    def drawAttributeGraph(self):\n",
    "        self.createAttributeGraph()\n",
    "        nx.draw_networkx(self.graph, connectionstyle = 'arc3, rad=0.2')\n",
    "        \n",
    "    def drawWeightedGraph(self):\n",
    "        self.createWeightedGraph()\n",
    "        nx.draw_networkx(self.graph, connectionstyle = 'arc3, rad=0.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_triples = [('Italy', 'Attack', 'EU'), ('EU', 'Support', 'Italy'), ('EU', 'Told', 'Italy'), ('EU', 'Announce', 'Measures'), ('Italy', 'Announce', 'Measures'), ('Italy', 'Open', 'Frontiers'), ('Italy', 'Lock', 'Frontiers'), ('Italy', 'Lock', 'Frontiers'), ('EU', 'Instantiated', 'Founds'), ('Monster', 'Attack', 'EU')]\n",
    "\n",
    "td_graph = TripleGraph(input_triples)\n",
    "\n",
    "td_graph.drawWeightedGraph()\n",
    "\n",
    "print(td_graph)\n",
    "\n",
    "#td_graph.getNodeOutgoingRelations('Italy')\n",
    "\n",
    "#custom_graph_from_triples.getNodeIngoingRelations('Italy')\n",
    "\n",
    "#custom_graph_from_triples.generateGraphWithWeightsFromTriples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
